import torch
import torch.nn as nn
import torch.nn.functional as nnf
from typing import Union
import numpy as np
import os
import cv2

from FLR_Module import FLR_Module


class conv(nn.Module):
    def __init__(
        self, 
        in_channels, 
        out_channels,
        kernel_size=3,
        stride=1,
        padding=1,
        has_bn=True, 
        has_relu=True,
        **kwargs
        ):
        super().__init__()
        self.conv = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            **kwargs)
        self.has_bn = has_bn
        if self.has_bn:
            self.bn = nn.BatchNorm2d(out_channels)
        self.has_relu = has_relu
        if self.has_relu:
            self.relu = nn.ReLU()

    def forward(self, x):
        x = self.conv(x)
        if self.has_bn:
            x = self.bn(x)
        if self.has_relu:
            x = self.relu(x)
        return x

class upconv(nn.Module):
    def __init__(self, in_channels, out_channels, ratio=2):
        super(upconv, self).__init__()
        self.elu = nn.ELU()
        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, bias=False, kernel_size=3, stride=1, padding=1)
        self.ratio = ratio
        
    def forward(self, x):
        up_x = nnf.interpolate(x, scale_factor=self.ratio, mode='nearest')
        out = self.conv(up_x)
        out = self.elu(out)
        return out

class resnext_block(nn.Module):
    def __init__(self, in_channels, out_channels, stride, groups, has_proj=False):
        super().__init__()
        bottleneck = out_channels//4
        assert (bottleneck % groups == 0) and (bottleneck / groups) % 4 == 0, (bottleneck, groups)
        self.conv_1x1_shrink = conv(in_channels, bottleneck, kernel_size=1, padding=0)
        self.conv_3x3        = conv(bottleneck,  bottleneck, kernel_size=3, stride=stride, groups=groups)
        self.conv_1x1_expand = conv(bottleneck,  out_channels, kernel_size=1, padding=0, has_relu=False) 

        self.has_proj = has_proj
        if self.has_proj:
            if stride == 2:
                self.dsp = nn.AvgPool2d(kernel_size=2, stride=2)
            self.shortcut = conv(in_channels, out_channels, kernel_size=1, padding=0, has_relu=False)
        self.relu = nn.ReLU()

    def forward(self, x):
        proj = x
        if self.has_proj:
            if hasattr(self, "dsp"):
                proj = self.dsp(proj)
            proj = self.shortcut(proj)
        x = self.conv_1x1_shrink(x)
        x = self.conv_3x3(x)
        x = self.conv_1x1_expand(x)
        x = x + proj
        x = self.relu(x)
        return x


class Encoder(nn.Module):
    def __init__(self, in_channels, base_ch=16):
        super().__init__()

        # full > 1/2 (conv 1)
        self.conv00 = conv(in_channels, base_ch, stride=2) # ì±„ë„ 3 > 16

        # 1/2 > 1/4 (Layer 1)
        self.res1_0 = resnext_block(base_ch, base_ch*2, stride=2, groups=2, has_proj=True) # ì±„ë„ 16 > 32
        self.res1_1 = resnext_block(base_ch*2, base_ch*2, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€
        self.res1_2 = resnext_block(base_ch*2, base_ch*2, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€
        
        # 1/4 > 1/8 (Layer 2)
        self.res2_0 = resnext_block(base_ch*2, base_ch*4, stride=2, groups=2, has_proj=True) # ì±„ë„ 32 > 64
        self.res2_1 = resnext_block(base_ch*4, base_ch*4, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€
        self.res2_2 = resnext_block(base_ch*4, base_ch*4, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€

        # 1/8 > 1/16 (Layer 3)
        self.res3_0 = resnext_block(base_ch*4, base_ch*8, stride=2, groups=2, has_proj=True) # ì±„ë„ 64 > 128
        self.res3_1 = resnext_block(base_ch*8, base_ch*8, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€
        self.res3_2 = resnext_block(base_ch*8, base_ch*8, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€

        # 1/16 > 1/32 (Layer 4)
        self.res4_0 = resnext_block(base_ch*8, base_ch*16, stride=2, groups=2, has_proj=True) # ì±„ë„ 128 > 256
        self.res4_1 = resnext_block(base_ch*16, base_ch*16, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€
        self.res4_2 = resnext_block(base_ch*16, base_ch*16, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€
        
    def forward(self,x):
        # full > 1/2
        x0 = self.conv00(x)

        # 1/2 > 1/4 
        x1= self.res1_0(x0)
        x1= self.res1_1(x1)
        x1= self.res1_2(x1)

        # 1/4 > 1/8
        x2= self.res2_0(x1)
        x2= self.res2_1(x2)
        x2= self.res2_2(x2)

        # 1/8 > 1/16
        x3= self.res3_0(x2)
        x3= self.res3_1(x3)
        x3= self.res3_2(x3)

        # 1/16 > 1/32
        x4= self.res4_0(x3)
        x4= self.res4_1(x4)
        x4= self.res4_2(x4)

        return x0, x1, x2, x3, x4

class Decoder(nn.Module):
    def __init__(self, base_ch=16, num_features=256, max_depth: float = 80.0):
        super().__init__()

        self.max_depth = max_depth

        H_2 = base_ch # 16
        H_4 = base_ch*2 # 32
        H_8 = base_ch*4 # 64
        H_16 = base_ch*8 # 128
        H_32 = base_ch*16 # 256

        # 1/32 > 1/16
        self.upconv5 = upconv(H_32, num_features) # ì±„ë„ 256 > 256 
        self.bn5        = nn.BatchNorm2d(num_features, momentum=0.01, affine=True, eps=1.1e-5)
        self.conv5      = torch.nn.Sequential(nn.Conv2d(num_features + H_16, num_features, 3, 1, 1, bias=False),
                                              nn.ELU())

        # 1/16 > 1/8
        self.upconv4    = upconv(num_features, num_features // 2) # ì±„ë„ 256 > 128
        self.bn4        = nn.BatchNorm2d(num_features // 2, momentum=0.01, affine=True, eps=1.1e-5)
        self.conv4      = torch.nn.Sequential(nn.Conv2d(num_features // 2 + H_8, num_features // 2, 3, 1, 1, bias=False),
                                              nn.ELU())
        self.bn4_2      = nn.BatchNorm2d(num_features // 2, momentum=0.01, affine=True, eps=1.1e-5)

        # 1/8 > 1/4
        self.upconv3    = upconv(num_features // 2, num_features // 4) # ì±„ë„ 128 > 64
        self.bn3        = nn.BatchNorm2d(num_features // 4, momentum=0.01, affine=True, eps=1.1e-5)
        self.conv3      = torch.nn.Sequential(nn.Conv2d(num_features // 4 + H_4, num_features // 4, 3, 1, 1, bias=False),
                                              nn.ELU())

        # 1/4 > 1/2
        self.upconv2    = upconv(num_features // 4, num_features // 8) # ì±„ë„ 64 > 32
        self.bn2        = nn.BatchNorm2d(num_features // 8, momentum=0.01, affine=True, eps=1.1e-5)
        self.conv2      = torch.nn.Sequential(nn.Conv2d(num_features // 8 + H_2, num_features // 8, 3, 1, 1, bias=False),
                                              nn.ELU())

        # 1/2 > Full
        self.upconv1    = upconv(num_features // 8, num_features // 16) # ì±„ë„ 32 > 16
        self.conv1      = torch.nn.Sequential(nn.Conv2d(num_features // 16 + 3, num_features // 16, 3, 1, 1, bias=False),
                                              nn.ELU())

        self.get_depth  = torch.nn.Sequential(nn.Conv2d(num_features // 16, 1, 3, 1, 1, bias=False),
                                              nn.Sigmoid())

        self.use_seg = True   # ì¼ë‹¨ ì¼œë‘ê³ , ë‚˜ì¤‘ì— argsë¡œ ë¹¼ë„ ë¨
        self.num_seg_classes = 8  # coarse 8-class

        if self.use_seg:
            # 1) H/4 â†’ H/2
            self.seg_upconv2 = upconv(num_features // 4, num_features // 8)
            self.seg_bn2     = nn.BatchNorm2d(num_features // 8, momentum=0.01, affine=True, eps=1.1e-5)
            self.seg_conv2   = torch.nn.Sequential(nn.Conv2d(num_features // 8 + H_4, num_features // 8, 3, 1, 1, bias=False),
                                                    nn.ELU())

            # 2) H/2 â†’ H
            self.seg_upconv1 = upconv(num_features // 8, num_features // 16)
            self.seg_conv1   = torch.nn.Sequential(nn.Conv2d(num_features // 16 + H_2, num_features // 16, 3, 1, 1, bias=False),
                                                    nn.ELU())

            # 3) ìµœì¢… seg head
            self.seg_head    = nn.Conv2d(
                num_features // 16,
                self.num_seg_classes,
                kernel_size=3,
                stride=1,
                padding=1,
                bias=False,
            )

        
    def forward(self, x0, x1, x2, x3, x4, x_img):
        # x4: H/32, x3: H/16, x2: H/8, x1: H/4, x0: H/2, x_img: H

        # 1) H/32 â†’ H/16
        u5 = self.upconv5(x4)          # [B, 256, H/16, W/16]
        u5 = self.bn5(u5)
        c5 = torch.cat([u5, x3], dim=1)   # [B, 256+128, H/16, W/16]
        f5 = self.conv5(c5)               # 256+128 ì—ì„œ 256ìœ¼ë¡œ ì±„ë„ ë§ì¶°ì£¼ê¸°

        # 2) H/16 â†’ H/8
        u4 = self.upconv4(f5)          # [B, 128, H/8, W/8]
        u4 = self.bn4(u4)
        c4 = torch.cat([u4, x2], dim=1)   # [B, 128+64, H/8, W/8]
        f4 = self.conv4(c4)               # 128+64ì—ì„œ 128ë¡œ ì±„ë„ ë§ì¶°ì£¼ê¸°
        f4 = self.bn4_2(f4)

        # 3) H/8 â†’ H/4
        u3 = self.upconv3(f4)          # [B, 64, H/4, W/4]
        u3 = self.bn3(u3)
        c3 = torch.cat([u3, x1], dim=1)   # [B, 64+32, H/4, W/4]
        f3 = self.conv3(c3)               # 64+32ì—ì„œ 64ë¡œ ì±„ë„ ë§ì¶°ì£¼ê¸°
        # H/4 í”¼ì²˜ ë°›ì•„ì„œ H/2ë¡œ ì—…ìƒ˜í”Œ (seg ë¸Œëœì¹˜ìš©)
        if self.use_seg:
            seg_u2 = self.seg_upconv2(f3)      # [B, Cseg, H/2, W/2]
            seg_u2 = self.seg_bn2(seg_u2)      

        # 4) H/4 â†’ H/2
        u2 = self.upconv2(f3)          # [B, 32, H/2, W/2]
        u2 = self.bn2(u2)
        c2 = torch.cat([u2, x0], dim=1)   # [B, 32+16, H/2, W/2]
        f2 = self.conv2(c2)               # [B, 32, H/2, W/2]

        # depth f2ë‘ ì—…ìƒ˜í”Œí•œ seg2 concat í›„ H/2 í”¼ì²˜ ë°›ì•„ì„œ H(full)ë¡œ ì—…ìƒ˜í”Œ
        if self.use_seg:
            f2_cat = torch.cat([f2, seg_u2], dim=1) # [B, 32+32, H/2, W/2]
            f2 = self.seg_conv2(f2_cat)  # 32+32ì—ì„œ 32ë¡œ ë§ì¶°ì£¼ê¸°

            seg_u1 = self.seg_upconv1(f2) 

        # 5) H/2 â†’ H
        u1 = self.upconv1(f2)          # [B, 16, H, W]

        assert x_img.shape[2:] == u1.shape[2:], \
            f"Shape mismatch: x_img={x_img.shape[2:]}, u1={u1.shape[2:]}"

        c1 = torch.cat([u1, x_img], dim=1)   # [B, 16+3, H, W]
        f1 = self.conv1(c1)                  # [B, 16, H, W]

        #depth f1ì´ë‘ seg1 concat
        if self.use_seg:
            f1_cat = torch.cat([f1, seg_u1], dim=1)   # [B, 16 + 16, H/2, W/2]
            f1 = self.seg_conv1(f1_cat)               # 16+16ì—ì„œ 16ìœ¼ë¡œ ë§ì¶°ì£¼ê¸°
            seg_logits = self.seg_head(f1) 

        depth_norm = self.get_depth(f1)      # [B, 1, H, W], 0~1
        depth = self.max_depth * depth_norm  # [B, 1, H, W], 0~max_depth

        return depth, seg_logits



class Network(nn.Module):
    def __init__(self):
        super().__init__()

        self.encoder = Encoder(in_channels=3, base_ch=16)
        self.decoder = Decoder(base_ch=16, num_features=256, max_depth=80.0)

        # ğŸ”¥ FLR teacher modules (feature-level regularizer)
        H_2 = 16
        H_4 = 32
        H_8 = 64
        H_16 = 128
        H_32 = 256 

        self.use_flr = True
        self.flr_1_2 = FLR_Module(H_2)
        self.flr_1_4  = FLR_Module(H_4)
        self.flr_1_8  = FLR_Module(H_8)
        self.flr_1_16 = FLR_Module(H_16)
        self.flr_1_32 = FLR_Module(H_32)

    def compute_llr_loss(self, pred, teacher, pp_inst, pp_valid_mask):
        """
        pred        : [B, 1, H, W]  (student depth)
        teacher     : [B, 1, H, W]  (teacher depth, DepthPro)
        pp_inst     : [B, H, W] or [B, 1, H, W]  (instance id map)
        pp_valid_mask : [B, 1, H, W]  (panoptic valid mask, 0/1)

        ì—­í• :
        - ê°™ì€ instance ì•ˆì—ì„œ 3x3 ì´ì›ƒ í”½ì…€ ìŒ (i,j)ì— ëŒ€í•´
          Î”s = D_s(i) - D_s(j)
          Î”t = D_t(i) - D_t(j)
          ì˜ ì°¨ì´ë¥¼ Smooth L1ë¡œ ë§ì¶”ëŠ” loss.
        """

        # pp_inst shape ì •ë¦¬: [B,H,W]
        if pp_inst.dim() == 4:
            assert pp_inst.size(1) == 1, f"pp_inst must be [B,1,H,W] or [B,H,W], got {pp_inst.shape}"
            inst = pp_inst[:, 0]  # [B,H,W]
        elif pp_inst.dim() == 3:
            inst = pp_inst
        else:
            raise ValueError(f"pp_inst must be [B,H,W] or [B,1,H,W], got {pp_inst.shape}")

        B, _, H, W = pred.shape

        # [B,1,H,W] -> [B,H,W]
        D_s = pred[:, 0]
        D_t = teacher[:, 0]
        valid_pan = (pp_valid_mask[:, 0] > 0.5)  # [B,H,W]

        # teacher depthê°€ 0 ì´ìƒì¸ ê³³ë§Œ ì‚¬ìš© (í´ë¦¬í•‘ ë²”ìœ„ ë‚´)
        teacher_valid = (D_t > 0.0)
        # ìµœì¢… valid mask: panoptic valid âˆ© teacher ìœ íš¨
        valid = valid_pan & teacher_valid  # [B,H,W]

        # ì•„ë¬´ ê²ƒë„ ì—†ìœ¼ë©´ 0 ë¦¬í„´
        if valid.sum() == 0:
            return pred.new_tensor(0.0)

        # 8-connected ë°©í–¥ (3x3 ì´ì›ƒ)
        directions = [
            (-1, -1), (-1, 0), (-1, 1),
            ( 0, -1),          ( 0, 1),
            ( 1, -1), ( 1, 0), ( 1, 1),
        ]

        total_loss = pred.new_tensor(0.0)
        total_count = pred.new_tensor(0.0)

        for dy, dx in directions:
            # rollë¡œ neighbor ê°€ì ¸ì˜¤ê¸°
            D_s_n = torch.roll(D_s, shifts=(dy, dx), dims=(1, 2))
            D_t_n = torch.roll(D_t, shifts=(dy, dx), dims=(1, 2))
            inst_n = torch.roll(inst, shifts=(dy, dx), dims=(1, 2))
            valid_n = torch.roll(valid, shifts=(dy, dx), dims=(1, 2))

            # ê°™ì€ instance & instance id != 0
            same_inst = (inst == inst_n) & (inst != 0)
            # center / neighbor ë‘˜ ë‹¤ valid
            both_valid = valid & valid_n
            # teacher neighborë„ ìœ íš¨
            teacher_valid_n = (D_t_n > 0.0)

            m = same_inst & both_valid & teacher_valid_n  # [B,H,W], bool

            # ê²½ê³„ wrap-around ì œê±° (rollë¡œ ìƒê¸°ëŠ” ì—£ì§€ ê°€ì§œ ì´ì›ƒ ì œê±°)
            border = torch.ones_like(m, dtype=torch.bool)

            # ì„¸ë¡œ ë°©í–¥ ê²½ê³„ ì œê±°
            if dy > 0:
                # ìœ„ìª½ dyì¤„ì´ rollë¡œ wrapëœ ë¶€ë¶„ â†’ ì œê±°
                border[:, :dy, :] = False
            elif dy < 0:
                # ì•„ë˜ìª½ |dy|ì¤„ì´ wrapëœ ë¶€ë¶„ â†’ ì œê±°
                border[:, H+dy:, :] = False  # dyëŠ” ìŒìˆ˜

            # ê°€ë¡œ ë°©í–¥ ê²½ê³„ ì œê±°
            if dx > 0:
                # ì™¼ìª½ dxì—´ì´ wrapëœ ë¶€ë¶„ â†’ ì œê±°
                border[:, :, :dx] = False
            elif dx < 0:
                # ì˜¤ë¥¸ìª½ |dx|ì—´ì´ wrapëœ ë¶€ë¶„ â†’ ì œê±°
                border[:, :, W+dx:] = False  # dxëŠ” ìŒìˆ˜

            m = m & border


            if m.sum() == 0:
                continue

            # gradient (ì°¨ë¶„)
            delta_s = D_s - D_s_n  # [B,H,W]
            delta_t = D_t - D_t_n  # [B,H,W]

            diff = (delta_s - delta_t)[m]  # [N]
            if diff.numel() == 0:
                continue

            abs_diff = diff.abs()
            smooth_mask = (abs_diff < 1.0).float()
            value = 0.5 * (diff ** 2) * smooth_mask + (abs_diff - 0.5) * (1.0 - smooth_mask)

            total_loss += value.sum()
            total_count += diff.numel()

        if total_count < 1.0:
            return pred.new_tensor(0.0)

        return total_loss / total_count
    # ===== LLR ë©”ì„œë“œ ë =====

    def forward(self, img):
        x0, x1, x2, x3, x4 = self.encoder(img)
        depth, seg_logits = self.decoder(x0, x1, x2, x3, x4, img)
        return depth, seg_logits

    def forward_with_feats(self, img):
        x0, x1, x2, x3, x4 = self.encoder(img)
        depth, seg_logits = self.decoder(x0, x1, x2, x3, x4, img)
        return depth, seg_logits, (x0, x1, x2, x3, x4)

    @staticmethod
    def make_torch_tensor(data:Union[np.ndarray, torch.Tensor], device, dtype) -> torch.Tensor:
        if isinstance(data, np.ndarray):
            if data.dtype == np.uint16:
                data = data.astype(np.float32)
            data = torch.from_numpy(data)
        if data.device != device:
            data = data.to(device)
        if data.dtype != dtype:
            data = data.to(dtype)
        return data

    @staticmethod
    def padding(data, divisor=32, dim=0):
        # dim: 0 for 2 axis, 1 for height, 2 for width
        if dim == 0:
            B, C, H, W = data.shape
            new_h, new_w = ((H + divisor - 1) // divisor) * divisor, ((W + divisor - 1) // divisor) * divisor
            new_data = torch.zeros((B, C, new_h, new_w), device=data.device, dtype=data.dtype)
            new_data[:, :, :H, :W] = data
        elif dim == 1:
            B, C, H, W = data.shape
            new_h, new_w = ((H + divisor - 1) // divisor) * divisor, ((W + divisor - 1) // divisor) * divisor
            new_data = torch.zeros((B, C, new_h, W), device=data.device, dtype=data.dtype)
            new_data[:, :, :H, :W] = data
        else:
            B, C, H, W = data.shape
            new_h, new_w = ((H + divisor - 1) // divisor) * divisor, ((W + divisor - 1) // divisor) * divisor
            new_data = torch.zeros((B, C, H, new_w), device=data.device, dtype=data.dtype)
            new_data[:, :, :H, :W] = data
        return new_data

    @property
    def device(self, ):
        device_set = set([p.device for p in self.parameters()])
        assert len(device_set) == 1
        return device_set.pop()

    @property
    def dtype(self, ):
        dtype_set = set([p.dtype for p in self.parameters()])
        assert len(dtype_set) == 1
        return dtype_set.pop() 


    def downsample_depthmap(self, inp, factor):
        assert factor<=1
        n,c,h,w = inp.shape
        new_h, new_w = int(h*factor), int(w*factor)
        out = torch.zeros((n,c,new_h,new_w), device=self.device)

        nonzero_idx = list(torch.nonzero(inp, as_tuple=True))
        nonzero_arr = inp[nonzero_idx]
        nonzero_idx[2] = (nonzero_idx[2]*factor).long()
        nonzero_idx[3] = (nonzero_idx[3]*factor).long()
        out[nonzero_idx] = nonzero_arr
        return out

    @staticmethod
    def get_loss_l1_smooth(pred, label, mask):
        pred        = pred.reshape(pred.shape[0], -1)
        label       = label.reshape(label.shape[0], -1)
        mask        = mask.reshape(mask.shape[0], -1)
        diff        = (pred - label) * mask
        smooth_mask = (diff.abs() < 1.0).float()
        value       = (0.5 * diff ** 2) * smooth_mask + (diff.abs() - 0.5) * (1.0 - smooth_mask)
        serr        = value.sum(axis=1).mean() / mask.sum()
        L           = 1.0  #  label.partial_shape[1]
        loss        = serr / L
        return loss

    


    def forward_train(self, mini_batch_data: dict):
        img = self.padding(self.make_torch_tensor(mini_batch_data["img"], self.device, self.dtype))
        label = self.padding(self.make_torch_tensor(mini_batch_data["label"], self.device, self.dtype))
        label_mask = self.padding(self.make_torch_tensor(mini_batch_data['label_mask'], self.device, self.dtype))
        pp_inst = self.padding(self.make_torch_tensor(mini_batch_data['pp_inst'], self.device, torch.long))
        pp_sem = self.padding(self.make_torch_tensor(mini_batch_data['pp_sem'], self.device, torch.long))
        pp_valid_mask = self.padding(self.make_torch_tensor(mini_batch_data['pp_valid_mask'], self.device, self.dtype))
        teacher = self.padding(self.make_torch_tensor(mini_batch_data["teacher_depth"], self.device, self.dtype))

        pred, seg_logits, (x0, x1, x2, x3, x4) = self.forward_with_feats(img)

        l1_loss = self.get_loss_l1_smooth(pred, label, label_mask)
        flr_loss = pred.new_tensor(0.0)
        distill_loss = pred.new_tensor(0.0)
        llr_loss = pred.new_tensor(0.0)
        seg_loss = pred.new_tensor(0.0)

        llr_valid_mask = (teacher > 0.0).float() * pp_valid_mask  # [B,1,H,W]
        llr_loss = self.compute_llr_loss(
                pred,          # student depth [B,1,H,W]
                teacher,       # teacher depth [B,1,H,W]
                pp_inst,       # [B,1,H,W] or [B,H,W]
                llr_valid_mask # [B,1,H,W]
            )

        # teacherê°€ 0 ì´ìƒì¸ í”½ì…€ë§Œ ì‚¬ìš© (0ì´ë©´ ì—†ëŠ” depthë¡œ ê°€ì •)
        teacher_valid = (teacher > 0.0).float()
        distill_loss = self.get_loss_l1_smooth(pred, teacher, teacher_valid)
        if (seg_logits is not None) and self.decoder.use_seg:
            # pp_sem: [B,1,H,W] or [B,H,W] -> [B,H,W]
            if pp_sem.dim() == 4:
                pp_sem_flat = pp_sem[:, 0, :, :]   # [B,H,W]
            else:
                pp_sem_flat = pp_sem               # [B,H,W]

            pp_sem_flat = pp_sem_flat.long()

            # valid mask: [B,1,H,W] -> [B,H,W]
            valid_seg = (pp_valid_mask[:, 0, :, :] > 0.5).float()  # [B,H,W]

            # CE loss per pixel: [B,H,W]
            ce_per_pixel = nnf.cross_entropy(
                seg_logits,          # [B,C,H,W], C = 8
                pp_sem_flat,         # [B,H,W], ê°’: {0~5, 255}
                reduction="none",
                ignore_index=255,
            )

            # valid ì˜ì—­ë§Œ í‰ê· 
            seg_loss = (ce_per_pixel * valid_seg).sum() / (valid_seg.sum() + 1e-6)

        # if self.use_flr:
        #     # pp_inst í˜•íƒœ ì •ë¦¬: [B,H,W] ë˜ëŠ” [B,1,H,W] ë‘˜ ë‹¤ í—ˆìš©
        #     if pp_inst.dim() == 4:
        #         assert pp_inst.size(1) == 1, f"pp_inst must be [B,1,H,W] or [B,H,W], got {pp_inst.shape}"
        #         pp_inst_feat = pp_inst[:, 0]          # [B,H,W]
        #     elif pp_inst.dim() == 3:
        #         pp_inst_feat = pp_inst                 # [B,H,W]
        #     else:
        #         raise ValueError(f"pp_inst must be [B,H,W] or [B,1,H,W], got {pp_inst.shape}")

        #     # ë¼ì´ë‹¤ ìœ íš¨ + íŒŒë†‰í‹± ìœ íš¨ ë‘˜ ë‹¤ ë§Œì¡±í•˜ëŠ” ì˜ì—­ë§Œ ì‚¬ìš©
        #     valid = pp_valid_mask.detach()     # [B,1,H,W]
            

        #     # ê° scaleì— ë§ê²Œ valid mask downsample
        #     # valid_1_2 = nnf.interpolate(valid, size=x0.shape[2:], mode='nearest')  # x0: 1/2
        #     #valid_1_4  = nnf.interpolate(valid, size=x1.shape[2:], mode='nearest')  # x1: 1/4
        #     #valid_1_8  = nnf.interpolate(valid, size=x2.shape[2:], mode='nearest')  # x2: 1/8
        #     #valid_1_16 = nnf.interpolate(valid, size=x3.shape[2:], mode='nearest')  # x3: 1/16
        #     #valid_1_32 = nnf.interpolate(valid, size=x4.shape[2:], mode='nearest')


        #     # ğŸ”¹ Teacher feature: FLR_Module(features_detach, pp_inst)
        #     #    - featureëŠ” detach: studentë§Œ teacherë¥¼ ë”°ë¼ê°€ê²Œ
        #     #    - FLR_Module íŒŒë¼ë¯¸í„°ëŠ” í•™ìŠµë¨ (detach ì•ˆ í•¨)
        #     #x0_T = self.flr_1_2(x0.detach(), pp_inst_feat)  # [B,16,H/2,W/2]
        #     #x1_T = self.flr_1_4(x1.detach(),  pp_inst_feat)   # [B,32,H/4,W/4]
        #     #x2_T = self.flr_1_8(x2.detach(),  pp_inst_feat)   # [B,64,H/8,W/8]
        #     #x3_T = self.flr_1_16(x3.detach(), pp_inst_feat)   # [B,128,H/16,W/16]
        #     #x4_T = self.flr_1_32(x4.detach(), pp_inst_feat)

        #     def feat_loss(student_f, teacher_f, vmask):
        #         """
        #         student_f : [B,C,H,W]
        #         teacher_f : [B,C,H,W]
        #         vmask     : [B,1,H,W] (0/1)
        #         Smooth L1 + valid mask
        #         """
        #         # [B,1,H,W] -> {0,1}
        #         v = (vmask > 0.5).float()

        #         # [B,C,H,W], vëŠ” ë¸Œë¡œë“œìºìŠ¤íŠ¸
        #         diff = (student_f - teacher_f) * v

        #         # Smooth L1 (Huber) ë¶€ë¶„
        #         abs_diff    = diff.abs()
        #         smooth_mask = (abs_diff < 1.0).float()

        #         value = 0.5 * (diff ** 2) * smooth_mask + \
        #                 (abs_diff - 0.5) * (1.0 - smooth_mask)

        #         # valid element ê°œìˆ˜ = (ê³µê°„ valid í”½ì…€ ìˆ˜) * ì±„ë„ ìˆ˜
        #         denom = v.sum() * student_f.shape[1] + 1e-6

        #         return value.sum() / denom

        #     # L_1_2 = feat_loss(x0, x0_T, valid_1_2)
        #     #L_1_4  = feat_loss(x1, x1_T, valid_1_4)
        #     #L_1_8  = feat_loss(x2, x2_T, valid_1_8)
        #     #L_1_16 = feat_loss(x3, x3_T, valid_1_16)
        #     # L_1_32 = feat_loss(x4, x4_T, valid_1_32)

        #     # ê°€ì¤‘ì¹˜ëŠ” ì¼ë‹¨ 0.1ì”©, ë‚˜ì¤‘ì— íŠœë‹
        #     #flr_loss = 1.0 * (L_1_8 + L_1_4)

        lambda_distill = 1
        lambda_llr = 0.7
        lambda_seg = 0.5

        loss = l1_loss + lambda_distill * distill_loss + lambda_llr * llr_loss + lambda_seg * seg_loss


        # ---------------------- ì‹œê°í™” (train_vis) ----------------------

        vis_dir  = mini_batch_data.get("vis_dir", None)
        vis_name = mini_batch_data.get("vis_name", None)

        if (vis_dir is not None) and (vis_name is not None):
            try:
                os.makedirs(vis_dir, exist_ok=True)

                # ì›ë³¸ í¬ê¸° (padding ì „)
                _, _, H0, W0 = mini_batch_data["img"].shape

                # ë°°ì¹˜ì—ì„œ ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ ì‚¬ìš©
                img_vis   = img[0].detach().cpu()[:, :H0, :W0]        # [3, H, W] BGR
                label_vis = label[0].detach().cpu()[:, :H0, :W0]      # [1, H, W]
                pred_vis  = pred[0].detach().cpu()[:, :H0, :W0]       # [1, H, W]

                # ---- 1) RGB ì´ë¯¸ì§€: ì •ê·œí™” ì—†ì´ raw BGR 0~255 ê¸°ì¤€ ----
                img_bgr = img_vis.permute(1, 2, 0).numpy()            # [H, W, 3]
                img_bgr = np.clip(img_bgr, 0, 255).astype(np.uint8)   # floatì¼ ìˆ˜ ìˆìœ¼ë‹ˆ uint8ë¡œ

                def depth_to_color(d_tensor, max_depth=80.0):
                    d = d_tensor.squeeze(0).numpy()   # [H, W], float

                    if np.all(d == 0):
                        norm = np.zeros_like(d, dtype=np.uint8)
                    else:
                        # 0~max_depthë¡œ í´ë¦¬í•‘í•˜ê³  ì •ê·œí™”
                        d_clipped = np.clip(d, 0, max_depth) / max_depth
                        # ê°€ê¹Œìš´=ë¹¨ê°•, ë¨¼=íŒŒë‘ì´ ë˜ë„ë¡ ë°˜ì „
                        d_inv = 1.0 - d_clipped
                        norm = (d_inv * 255).astype(np.uint8)

                    color = cv2.applyColorMap(norm, cv2.COLORMAP_JET)
                    return color

                gt_color     = depth_to_color(label_vis)      # LiDAR GT depth
                pred_color   = depth_to_color(pred_vis)       # Student pred depth

                # ğŸ”¹ 1) Teacher depth ì‹œê°í™”
                teacher_vis  = teacher[0].detach().cpu()[:, :H0, :W0]     # [1,H,W]
                teacher_color = depth_to_color(teacher_vis)               # [H,W,3]

                # ğŸ”¹ 2) Panoptic semantic ì‹œê°í™”
                pp_sem_vis   = pp_sem[0].detach().cpu()[:, :H0, :W0]      # [1,H,W]
                pp_vm_vis    = pp_valid_mask[0].detach().cpu()[:, :H0, :W0]  # [1, H, W]

                sem = pp_sem_vis.squeeze(0).numpy().astype(np.int32)
                vm  = pp_vm_vis.squeeze(0).numpy().astype(bool)

                sem_vis = sem.copy()
                sem_vis[sem_vis == 255] = 0    # void ì œê±°

                if sem_vis.max() == 0:
                    sem_color = np.zeros((H0, W0, 3), dtype=np.uint8)
                else:
                    max_valid = sem_vis.max()  # ì—¬ê¸°ì„œëŠ” 5 ì •ë„
                    sem_norm = (sem_vis.astype(np.float32) / max_valid) * 255.0
                    sem_norm = sem_norm.astype(np.uint8)
                    sem_color = cv2.applyColorMap(sem_norm, cv2.COLORMAP_JET)
                    sem_color[~vm] = 0


                # ---- 3) ë„¤ ì¥ì„ ì˜†ìœ¼ë¡œ ì´ì–´ë¶™ì´ê¸° ----
                panel = np.concatenate(
                    [img_bgr, sem_color, teacher_color, pred_color], axis=1
                )

                out_path = os.path.join(vis_dir, vis_name)
                cv2.imwrite(out_path, panel)

            except Exception as e:
                # ì‹œê°í™” ìª½ ë¬¸ì œ ë•Œë¬¸ì— í•™ìŠµì´ ë©ˆì¶”ì§€ ì•Šë„ë¡ ì¡°ìš©íˆ ë¬´ì‹œ
                pass

        with torch.no_grad():
            mae = (torch.abs(pred - label) * label_mask).sum() / (label_mask.sum() + 1e-6)
            rmse = torch.sqrt(((pred - label) ** 2 * label_mask).sum() / (label_mask.sum() + 1e-6))

        monitors = {
            "train_loss":  loss.detach(),
            "mae":         mae.detach(),
            "rmse":        rmse.detach(),
            "flr_loss":    flr_loss.detach(),
            "distill_loss":distill_loss.detach(),
            "llr_loss":    llr_loss.detach(),
            "seg_loss":    seg_loss.detach(),
        }

        return loss, monitors

        # ----------------------------------------------------------------

    def forward_eval(self, mini_batch_data: dict):
        """
        valid / test DataLoaderì—ì„œ ë‚˜ì˜¨ mini_batch_data í•˜ë‚˜ë¥¼ ë°›ì•„ì„œ
        - padding + device/dtype ë§ì¶”ê³ 
        - ëª¨ë¸ ì¶”ë¡ 
        - ì›ë˜ H, Wë¡œ í¬ë¡­í•´ì„œ dictë¡œ ë°˜í™˜
        """
        # ì›ë³¸ í¬ê¸° (padding í•˜ê¸° ì „ sizeë¥¼ ê¸°ì–µí•´ ë‘ )
        B, C, H, W = mini_batch_data["img"].shape

        # í•„ìˆ˜ í•­ëª©
        img        = self.padding(self.make_torch_tensor(mini_batch_data["img"],   self.device, self.dtype))
        label      = self.padding(self.make_torch_tensor(mini_batch_data["label"], self.device, self.dtype))
        label_mask = self.padding(self.make_torch_tensor(mini_batch_data["label_mask"], self.device, self.dtype))
        #pp_inst = self.padding(self.make_torch_tensor(mini_batch_data['pp_inst'], self.device, torch.long))
        #pp_sem = self.padding(self.make_torch_tensor(mini_batch_data['pp_sem'], self.device, torch.long))
        #pp_valid_mask = self.padding(self.make_torch_tensor(mini_batch_data['pp_valid_mask'], self.device, self.dtype))

 
        with torch.no_grad():
            # ë‹¨ì¼ ìŠ¤ì¼€ì¼ depth ì˜ˆì¸¡
             pred, _ = self.forward(img)   # [B, 1, H_pad, W_pad]

        out = {
            "pred":       pred[:, :, :H, :W],       # [B, 1, H, W]
            "img":        img[:, :, :H, :W],        # [B, 3, H, W]
            "label":      label[:, :, :H, :W],      # [B, 1, H, W]
            "label_mask": label_mask[:, :, :H, :W], # [B, 1, H, W]
        }

        return out

    def forward_test(self,img):
        """
        img: [B, 3, H, W] (np.ndarray or torch.Tensor)
        return: depth_pred: [B, 1, H, W] (numpy)
        """
        B, C, H, W = img.shape

        img = self.padding(self.make_torch_tensor(img, self.device, self.dtype))

        with torch.no_grad():
             pred, _ = self.forward(img)   # [B, 1, H_pad, W_pad]

        # padding ì „ì— ì›ë˜ í¬ê¸°ë§Œ ì˜ë¼ì„œ numpyë¡œ ë°˜í™˜
        pred = pred[:, :, :H, :W].detach().cpu().numpy()
        return pred

