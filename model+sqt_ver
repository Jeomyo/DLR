import torch
import torch.nn as nn
import torch.nn.functional as nnf
from typing import Union
import numpy as np
import os
import cv2

from FLR_Module import FLR_Module


class conv(nn.Module):
    def __init__(
        self, 
        in_channels, 
        out_channels,
        kernel_size=3,
        stride=1,
        padding=1,
        has_bn=True, 
        has_relu=True,
        **kwargs
        ):
        super().__init__()
        self.conv = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            **kwargs)
        self.has_bn = has_bn
        if self.has_bn:
            self.bn = nn.BatchNorm2d(out_channels)
        self.has_relu = has_relu
        if self.has_relu:
            self.relu = nn.ReLU()

    def forward(self, x):
        x = self.conv(x)
        if self.has_bn:
            x = self.bn(x)
        if self.has_relu:
            x = self.relu(x)
        return x

class upconv(nn.Module):
    def __init__(self, in_channels, out_channels, ratio=2):
        super(upconv, self).__init__()
        self.elu = nn.ELU()
        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, bias=False, kernel_size=3, stride=1, padding=1)
        self.ratio = ratio
        
    def forward(self, x):
        up_x = nnf.interpolate(x, scale_factor=self.ratio, mode='nearest')
        out = self.conv(up_x)
        out = self.elu(out)
        return out

class resnext_block(nn.Module):
    def __init__(self, in_channels, out_channels, stride, groups, has_proj=False):
        super().__init__()
        bottleneck = out_channels//4
        assert (bottleneck % groups == 0) and (bottleneck / groups) % 4 == 0, (bottleneck, groups)
        self.conv_1x1_shrink = conv(in_channels, bottleneck, kernel_size=1, padding=0)
        self.conv_3x3        = conv(bottleneck,  bottleneck, kernel_size=3, stride=stride, groups=groups)
        self.conv_1x1_expand = conv(bottleneck,  out_channels, kernel_size=1, padding=0, has_relu=False) 

        self.has_proj = has_proj
        if self.has_proj:
            if stride == 2:
                self.dsp = nn.AvgPool2d(kernel_size=2, stride=2)
            self.shortcut = conv(in_channels, out_channels, kernel_size=1, padding=0, has_relu=False)
        self.relu = nn.ReLU()

    def forward(self, x):
        proj = x
        if self.has_proj:
            if hasattr(self, "dsp"):
                proj = self.dsp(proj)
            proj = self.shortcut(proj)
        x = self.conv_1x1_shrink(x)
        x = self.conv_3x3(x)
        x = self.conv_1x1_expand(x)
        x = x + proj
        x = self.relu(x)
        return x


class Encoder(nn.Module):
    def __init__(self, in_channels, base_ch=16):
        super().__init__()

        # full > 1/2 (conv 1)
        self.conv00 = conv(in_channels, base_ch, stride=2) # ì±„ë„ 3 > 16

        # 1/2 > 1/4 (Layer 1)
        self.res1_0 = resnext_block(base_ch, base_ch*2, stride=2, groups=2, has_proj=True) # ì±„ë„ 16 > 32
        self.res1_1 = resnext_block(base_ch*2, base_ch*2, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€
        self.res1_2 = resnext_block(base_ch*2, base_ch*2, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€
        
        # 1/4 > 1/8 (Layer 2)
        self.res2_0 = resnext_block(base_ch*2, base_ch*4, stride=2, groups=2, has_proj=True) # ì±„ë„ 32 > 64
        self.res2_1 = resnext_block(base_ch*4, base_ch*4, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€
        self.res2_2 = resnext_block(base_ch*4, base_ch*4, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€

        # 1/8 > 1/16 (Layer 3)
        self.res3_0 = resnext_block(base_ch*4, base_ch*8, stride=2, groups=2, has_proj=True) # ì±„ë„ 64 > 128
        self.res3_1 = resnext_block(base_ch*8, base_ch*8, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€
        self.res3_2 = resnext_block(base_ch*8, base_ch*8, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€

        # 1/16 > 1/32 (Layer 4)
        self.res4_0 = resnext_block(base_ch*8, base_ch*16, stride=2, groups=2, has_proj=True) # ì±„ë„ 128 > 256
        self.res4_1 = resnext_block(base_ch*16, base_ch*16, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€
        self.res4_2 = resnext_block(base_ch*16, base_ch*16, stride=1, groups=2, has_proj=False) # í•´ìƒë„, ì±„ë„ ìœ ì§€ ë° í‘œí˜„ë ¥ ì¦ê°€
        
    def forward(self,x):
        # full > 1/2
        x0 = self.conv00(x)

        # 1/2 > 1/4 
        x1= self.res1_0(x0)
        x1= self.res1_1(x1)
        x1= self.res1_2(x1)

        # 1/4 > 1/8
        x2= self.res2_0(x1)
        x2= self.res2_1(x2)
        x2= self.res2_2(x2)

        # 1/8 > 1/16
        x3= self.res3_0(x2)
        x3= self.res3_1(x3)
        x3= self.res3_2(x3)

        # 1/16 > 1/32
        x4= self.res4_0(x3)
        x4= self.res4_1(x4)
        x4= self.res4_2(x4)

        return x0, x1, x2, x3, x4

class Decoder(nn.Module):
    def __init__(self, base_ch=16, num_features=256, max_depth: float = 80.0):
        super().__init__()

        self.max_depth = max_depth

        H_2 = base_ch # 16
        H_4 = base_ch*2 # 32
        H_8 = base_ch*4 # 64
        H_16 = base_ch*8 # 128
        H_32 = base_ch*16 # 256

        # 1/32 > 1/16
        self.upconv5 = upconv(H_32, num_features) # ì±„ë„ 256 > 256 
        self.bn5        = nn.BatchNorm2d(num_features, momentum=0.01, affine=True, eps=1.1e-5)
        self.conv5      = torch.nn.Sequential(nn.Conv2d(num_features + H_16, num_features, 3, 1, 1, bias=False),
                                              nn.ELU())

        # 1/16 > 1/8
        self.upconv4    = upconv(num_features, num_features // 2) # ì±„ë„ 256 > 128
        self.bn4        = nn.BatchNorm2d(num_features // 2, momentum=0.01, affine=True, eps=1.1e-5)
        self.conv4      = torch.nn.Sequential(nn.Conv2d(num_features // 2 + H_8, num_features // 2, 3, 1, 1, bias=False),
                                              nn.ELU())
        self.bn4_2      = nn.BatchNorm2d(num_features // 2, momentum=0.01, affine=True, eps=1.1e-5)

        # 1/8 > 1/4
        self.upconv3    = upconv(num_features // 2, num_features // 4) # ì±„ë„ 128 > 64
        self.bn3        = nn.BatchNorm2d(num_features // 4, momentum=0.01, affine=True, eps=1.1e-5)
        self.conv3      = torch.nn.Sequential(nn.Conv2d(num_features // 4 + H_4, num_features // 4, 3, 1, 1, bias=False),
                                              nn.ELU())

        # 1/4 > 1/2
        self.upconv2    = upconv(num_features // 4, num_features // 8) # ì±„ë„ 64 > 32
        self.bn2        = nn.BatchNorm2d(num_features // 8, momentum=0.01, affine=True, eps=1.1e-5)
        self.conv2      = torch.nn.Sequential(nn.Conv2d(num_features // 8 + H_2, num_features // 8, 3, 1, 1, bias=False),
                                              nn.ELU())

        # 1/2 > Full
        self.upconv1    = upconv(num_features // 8, num_features // 16) # ì±„ë„ 32 > 16
        self.conv1      = torch.nn.Sequential(nn.Conv2d(num_features // 16 + 3, num_features // 16, 3, 1, 1, bias=False),
                                              nn.ELU())

        self.get_depth  = torch.nn.Sequential(nn.Conv2d(num_features // 16, 1, 3, 1, 1, bias=False),
                                              nn.Sigmoid())

        
    def forward(self, x0, x1, x2, x3, x4, x_img):
        # x4: H/32, x3: H/16, x2: H/8, x1: H/4, x0: H/2, x_img: H

        # 1) H/32 â†’ H/16
        u5 = self.upconv5(x4)          # [B, 256, H/16, W/16]
        u5 = self.bn5(u5)
        c5 = torch.cat([u5, x3], dim=1)   # [B, 256+128, H/16, W/16]
        f5 = self.conv5(c5)               # [B, 256, H/16, W/16]

        # 2) H/16 â†’ H/8
        u4 = self.upconv4(f5)          # [B, 128, H/8, W/8]
        u4 = self.bn4(u4)
        c4 = torch.cat([u4, x2], dim=1)   # [B, 128+64, H/8, W/8]
        f4 = self.conv4(c4)               # [B, 128, H/8, W/8]
        f4 = self.bn4_2(f4)

        # 3) H/8 â†’ H/4
        u3 = self.upconv3(f4)          # [B, 64, H/4, W/4]
        u3 = self.bn3(u3)
        c3 = torch.cat([u3, x1], dim=1)   # [B, 64+32, H/4, W/4]
        f3 = self.conv3(c3)               # [B, 64, H/4, W/4]

        # 4) H/4 â†’ H/2
        u2 = self.upconv2(f3)          # [B, 32, H/2, W/2]
        u2 = self.bn2(u2)
        c2 = torch.cat([u2, x0], dim=1)   # [B, 32+16, H/2, W/2]
        f2 = self.conv2(c2)               # [B, 32, H/2, W/2]

        # 5) H/2 â†’ H
        u1 = self.upconv1(f2)          # [B, 16, H, W]

        assert x_img.shape[2:] == u1.shape[2:], \
            f"Shape mismatch: x_img={x_img.shape[2:]}, u1={u1.shape[2:]}"

        c1 = torch.cat([u1, x_img], dim=1)   # [B, 16+3, H, W]
        f1 = self.conv1(c1)                  # [B, 16, H, W]

        depth_norm = self.get_depth(f1)      # [B, 1, H, W], 0~1
        depth = self.max_depth * depth_norm  # [B, 1, H, W], 0~max_depth

        return depth



class Network(nn.Module):
    def __init__(self):
        super().__init__()

        self.encoder = Encoder(in_channels=3, base_ch=16)
        self.decoder = Decoder(base_ch=16, num_features=256, max_depth=80.0)

        # ğŸ”¥ FLR teacher modules (feature-level regularizer)
        H_2 = 16
        H_4 = 32
        H_8 = 64
        H_16 = 128
        H_32 = 256 

        self.use_flr = True
        # 1/2: ì¼ë‹¨ ì•ˆ ì”€ (ì›í•˜ë©´ ë‚˜ì¤‘ì— í™œì„±í™”)
        self.flr_1_2  = FLR_Module(
            channels=H_2,
            kernel_size=3,    # ì‘ì€ íŒ¨ì¹˜
            sigma_sp=1.0,
            sigma_feat=1.0,
        )

        # 1/4: ê°ì²´ êµ¬ì¡° ê°€ì¥ ì¤‘ìš” â€“ ë„“ê³  ë¶€ë“œëŸ½ê²Œ ë³´ì •
        self.flr_1_4  = FLR_Module(
            channels=H_4,
            kernel_size=5,    # ë„“ì€ íŒ¨ì¹˜
            sigma_sp=1.5,     # ê³µê°„ì ìœ¼ë¡œ ì¢€ ë” ì™„ë§Œ
            sigma_feat=1.0,   # feature metric ì •ê·œí™”
        )

        # 1/8: ì¡°ê¸ˆ ë” global êµ¬ì¡°ì§€ë§Œ, LDL ìœ„í—˜ â†‘ â†’ íŒ¨ì¹˜ ì‘ê²Œ
        self.flr_1_8  = FLR_Module(
            channels=H_8,
            kernel_size=3,    # 3x3ë¡œ ë” ë¡œì»¬
            sigma_sp=1.0,
            sigma_feat=1.0,
        )

        # 1/16, 1/32: ì¼ë‹¨ ë³´ë¥˜ìš© (ì›í•˜ë©´ ë‚˜ì¤‘ì— ì¼œê¸°)
        self.flr_1_16 = FLR_Module(
            channels=H_16,
            kernel_size=3,
            sigma_sp=1.0,
            sigma_feat=1.0,
        )
        self.flr_1_32 = FLR_Module(
            channels=H_32,
            kernel_size=3,
            sigma_sp=1.0,
            sigma_feat=1.0,
        )

    def forward(self, img) -> torch.Tensor:
        x0, x1, x2, x3, x4 = self.encoder(img)
        depth = self.decoder(x0, x1, x2, x3, x4, img)
        return depth

    def forward_with_feats(self, img) -> torch.Tensor:
        x0, x1, x2, x3, x4 = self.encoder(img)
        depth = self.decoder(x0, x1, x2, x3, x4, img)
        return depth, (x0, x1, x2, x3, x4)

    @staticmethod
    def make_torch_tensor(data:Union[np.ndarray, torch.Tensor], device, dtype) -> torch.Tensor:
        if isinstance(data, np.ndarray):
            if data.dtype == np.uint16:
                data = data.astype(np.float32)
            data = torch.from_numpy(data)
        if data.device != device:
            data = data.to(device)
        if data.dtype != dtype:
            data = data.to(dtype)
        return data

    @staticmethod
    def padding(data, divisor=32, dim=0):
        # dim: 0 for 2 axis, 1 for height, 2 for width
        if dim == 0:
            B, C, H, W = data.shape
            new_h, new_w = ((H + divisor - 1) // divisor) * divisor, ((W + divisor - 1) // divisor) * divisor
            new_data = torch.zeros((B, C, new_h, new_w), device=data.device, dtype=data.dtype)
            new_data[:, :, :H, :W] = data
        elif dim == 1:
            B, C, H, W = data.shape
            new_h, new_w = ((H + divisor - 1) // divisor) * divisor, ((W + divisor - 1) // divisor) * divisor
            new_data = torch.zeros((B, C, new_h, W), device=data.device, dtype=data.dtype)
            new_data[:, :, :H, :W] = data
        else:
            B, C, H, W = data.shape
            new_h, new_w = ((H + divisor - 1) // divisor) * divisor, ((W + divisor - 1) // divisor) * divisor
            new_data = torch.zeros((B, C, H, new_w), device=data.device, dtype=data.dtype)
            new_data[:, :, :H, :W] = data
        return new_data

    @property
    def device(self, ):
        device_set = set([p.device for p in self.parameters()])
        assert len(device_set) == 1
        return device_set.pop()

    @property
    def dtype(self, ):
        dtype_set = set([p.dtype for p in self.parameters()])
        assert len(dtype_set) == 1
        return dtype_set.pop() 

    @staticmethod
    def compute_affinity(feature, kernel_size):
        pad = kernel_size // 2
        feature = nnf.normalize(feature, dim=1)
        unfolded = nnf.pad(feature, [pad] * 4).unfold(2, kernel_size, 1).unfold(3, kernel_size, 1)
        feature = feature.unsqueeze(-1).unsqueeze(-1)
        similarity = (feature * unfolded).sum(dim=1, keepdim=True)
        affinity = torch.clamp(2 - 2 * similarity, min=1e-9).sqrt()
        return affinity


    def downsample_depthmap(self, inp, factor):
        assert factor<=1
        n,c,h,w = inp.shape
        new_h, new_w = int(h*factor), int(w*factor)
        out = torch.zeros((n,c,new_h,new_w), device=self.device)

        nonzero_idx = list(torch.nonzero(inp, as_tuple=True))
        nonzero_arr = inp[nonzero_idx]
        nonzero_idx[2] = (nonzero_idx[2]*factor).long()
        nonzero_idx[3] = (nonzero_idx[3]*factor).long()
        out[nonzero_idx] = nonzero_arr
        return out

    @staticmethod
    def get_loss_l1_smooth(pred, label, mask):
        pred        = pred.reshape(pred.shape[0], -1)
        label       = label.reshape(label.shape[0], -1)
        mask        = mask.reshape(mask.shape[0], -1)
        diff        = (pred - label) * mask
        smooth_mask = (diff.abs() < 1.0).float()
        value       = (0.5 * diff ** 2) * smooth_mask + (diff.abs() - 0.5) * (1.0 - smooth_mask)
        serr        = value.sum(axis=1).mean() / mask.sum()
        L           = 1.0  #  label.partial_shape[1]
        loss        = serr / L
        return loss

    def compute_sgt_loss(self, pp_inst, outputs):
        seg_target = pp_inst          # [B,1,H,W] or [B,H,W]
        if seg_target.dim() == 3:
            seg_target = seg_target.unsqueeze(1)
        N, _, H, W = seg_target.shape
        total_loss = 0.0

        # ì›ë˜ opt.sgt_scales / opt.sgt_kernel_size ëŒ€ì‹  í•˜ë“œì½”ë”©
        sgt_scales = [3, 2, 1]
        sgt_kernel_sizes = [5, 5, 5]
        sgt_margin = 0.35
        sgt_iso_margin = 0.65
        disable_hardest_neg = False
        disable_isolated_triplet = False

        # ë„ˆ ëª¨ë¸ ê¸°ì¤€: ì…ë ¥ í¬ê¸° H,WëŠ” padding í›„ img.shape[2:]
        height, width = H, W

        for s, kernel_size in zip(sgt_scales, sgt_kernel_sizes):
            pad = kernel_size // 2
            h, w = height // (2 ** s), width // (2 ** s)

            seg = nnf.interpolate(seg_target.float(), size=(h, w), mode='nearest')
            seg_pad = nnf.pad(seg, pad=[pad] * 4, value=-1)
            patches = seg_pad.unfold(2, kernel_size, 1).unfold(3, kernel_size, 1)
            aggregated_label = patches - seg.unsqueeze(-1).unsqueeze(-1)
            pos_idx = (aggregated_label == 0).float()
            neg_idx = (aggregated_label != 0).float()
            pos_num = pos_idx.sum(dim=(-1, -2))
            neg_num = neg_idx.sum(dim=(-1, -2))

            is_boundary = (pos_num >= kernel_size - 1) & (neg_num >= kernel_size - 1)

            feature = outputs[('d_feature', s)]    # ë¯¸ë¦¬ sgt_outputsì— ë„£ì–´ë‘” x0/x1/x2
            affinity = self.compute_affinity(feature, kernel_size=kernel_size)
            neg_dist = neg_idx * affinity

            if not disable_hardest_neg:
                neg_dist[neg_dist == 0] = 1e3
                neg_dist_x, arg_min_x = torch.min(neg_dist, dim=-1)
                neg_dist, arg_min_y = torch.min(neg_dist_x, dim=-1)
                arg_min_x = torch.gather(arg_min_x, -1,
                                        arg_min_y.unsqueeze(-1)).squeeze(-1)
                neg_dist = neg_dist[is_boundary]
            else:
                neg_dist = neg_dist.sum(dim=(-1, -2))[is_boundary] / \
                        (neg_num[is_boundary] + 1e-6)

            pos_dist = ((pos_idx * affinity).sum(dim=(-1, -2)) / (pos_num + 1e-6))[is_boundary]

            zeros = torch.zeros_like(pos_dist, device=feature.device)
            if not disable_isolated_triplet:
                loss = pos_dist + torch.max(zeros, sgt_iso_margin - neg_dist)
            else:
                loss = torch.max(zeros, sgt_margin + pos_dist - neg_dist)

            total_loss = total_loss + loss.mean() / (2 ** s)

        return total_loss


    def forward_train(self, mini_batch_data: dict):
        img = self.padding(self.make_torch_tensor(mini_batch_data["img"], self.device, self.dtype))
        label = self.padding(self.make_torch_tensor(mini_batch_data["label"], self.device, self.dtype))
        label_mask = self.padding(self.make_torch_tensor(mini_batch_data['label_mask'], self.device, self.dtype))
        pp_inst = self.padding(self.make_torch_tensor(mini_batch_data['pp_inst'], self.device, torch.long))
        #pp_sem = self.padding(self.make_torch_tensor(mini_batch_data['pp_sem'], self.device, torch.long))
        pp_valid_mask = self.padding(self.make_torch_tensor(mini_batch_data['pp_valid_mask'], self.device, self.dtype))

        pred, (x0, x1, x2, x3, x4) = self.forward_with_feats(img)

        l1_loss = self.get_loss_l1_smooth(pred, label, label_mask)
        flr_loss = pred.new_tensor(0.0)

        sgt_outputs = {}

        # s=3 â†’ 1/8 scale â†’ x2
        sgt_outputs[('d_feature', 3)] = x2
        # s=2 â†’ 1/4 scale â†’ x1
        sgt_outputs[('d_feature', 2)] = x1
        # s=1 â†’ 1/2 scale â†’ x0
        sgt_outputs[('d_feature', 1)] = x0

        if self.use_flr:
            # pp_inst í˜•íƒœ ì •ë¦¬: [B,H,W] ë˜ëŠ” [B,1,H,W] ë‘˜ ë‹¤ í—ˆìš©
            if pp_inst.dim() == 4:
                assert pp_inst.size(1) == 1, f"pp_inst must be [B,1,H,W] or [B,H,W], got {pp_inst.shape}"
                pp_inst_feat = pp_inst[:, 0]          # [B,H,W]
            elif pp_inst.dim() == 3:
                pp_inst_feat = pp_inst                 # [B,H,W]
            else:
                raise ValueError(f"pp_inst must be [B,H,W] or [B,1,H,W], got {pp_inst.shape}")

            # ë¼ì´ë‹¤ ìœ íš¨ + íŒŒë†‰í‹± ìœ íš¨ ë‘˜ ë‹¤ ë§Œì¡±í•˜ëŠ” ì˜ì—­ë§Œ ì‚¬ìš©
            valid = pp_valid_mask.detach()     # [B,1,H,W]
            

            # ê° scaleì— ë§ê²Œ valid mask downsample
            # valid_1_2 = nnf.interpolate(valid, size=x0.shape[2:], mode='nearest')  # x0: 1/2
            valid_1_4  = nnf.interpolate(valid, size=x1.shape[2:], mode='nearest')  # x1: 1/4
            valid_1_8  = nnf.interpolate(valid, size=x2.shape[2:], mode='nearest')  # x2: 1/8
            #valid_1_16 = nnf.interpolate(valid, size=x3.shape[2:], mode='nearest')  # x3: 1/16
            #valid_1_32 = nnf.interpolate(valid, size=x4.shape[2:], mode='nearest')


            # ğŸ”¹ Teacher feature: FLR_Module(features_detach, pp_inst)
            #    - featureëŠ” detach: studentë§Œ teacherë¥¼ ë”°ë¼ê°€ê²Œ
            #    - FLR_Module íŒŒë¼ë¯¸í„°ëŠ” í•™ìŠµë¨ (detach ì•ˆ í•¨)
            # x0_T = self.flr_1_2(x0.detach(), pp_inst_feat)  # [B,16,H/2,W/2]
            x1_T = self.flr_1_4(x1.detach(),  pp_inst_feat)   # [B,32,H/4,W/4]
            x2_T = self.flr_1_8(x2.detach(),  pp_inst_feat)   # [B,64,H/8,W/8]
            #x3_T = self.flr_1_16(x3.detach(), pp_inst_feat)   # [B,128,H/16,W/16]
            #x4_T = self.flr_1_32(x4.detach(), pp_inst_feat)

            def feat_loss(student_f, teacher_f, vmask):
                """
                student_f : [B,C,H,W]
                teacher_f : [B,C,H,W]
                vmask     : [B,1,H,W] (0/1)
                """
                v = (vmask > 0.5).float()          # ì•ˆì „í•˜ê²Œ binary
                diff = (student_f - teacher_f) * v
                # ì±„ë„ê¹Œì§€ í¬í•¨í•´ì„œ valid ìœ„ì¹˜ ì „ì²´ì— ëŒ€í•œ í‰ê·  L1
                denom = v.sum() * student_f.shape[1] + 1e-6
                return diff.abs().sum() / denom

            # L_1_2 = feat_loss(x0, x0_T, valid_1_2)
            L_1_4  = feat_loss(x1, x1_T, valid_1_4)
            L_1_8  = feat_loss(x2, x2_T, valid_1_8)
            #L_1_16 = feat_loss(x3, x3_T, valid_1_16)
            # L_1_32 = feat_loss(x4, x4_T, valid_1_32)

            # ê°€ì¤‘ì¹˜ëŠ” ì¼ë‹¨ 0.1ì”©, ë‚˜ì¤‘ì— íŠœë‹
            flr_loss = 0.05 * (L_1_8 + L_1_4)

        lambda_sgt = 0.1
        sgt_loss = pred.new_tensor(0.0)
        try:
            sgt_loss = self.compute_sgt_loss(pp_inst, sgt_outputs)
        except Exception:
            # í˜¹ì‹œë¼ë„ í¬ê¸° mismatchë‚˜ ë²„ê·¸ í„°ì ¸ë„ í•™ìŠµ ì•ˆ ëŠê¸°ê²Œ
            sgt_loss = pred.new_tensor(0.0)

        # ìµœì¢… loss = ê¸°ë³¸ depth loss + FLR feature ì •ê·œí™” + SGT metric loss

        loss = l1_loss + flr_loss + lambda_sgt * sgt_loss

        # ---------------------- ì‹œê°í™” (train_vis) ----------------------

        vis_dir  = mini_batch_data.get("vis_dir", None)
        vis_name = mini_batch_data.get("vis_name", None)

        if (vis_dir is not None) and (vis_name is not None):
            try:
                os.makedirs(vis_dir, exist_ok=True)

                # ì›ë³¸ í¬ê¸° (padding ì „)
                _, _, H0, W0 = mini_batch_data["img"].shape

                # ë°°ì¹˜ì—ì„œ ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ ì‚¬ìš©
                img_vis   = img[0].detach().cpu()[:, :H0, :W0]        # [3, H, W] BGR
                label_vis = label[0].detach().cpu()[:, :H0, :W0]      # [1, H, W]
                pred_vis  = pred[0].detach().cpu()[:, :H0, :W0]       # [1, H, W]

                # ---- 1) RGB ì´ë¯¸ì§€: ì •ê·œí™” ì—†ì´ raw BGR 0~255 ê¸°ì¤€ ----
                img_bgr = img_vis.permute(1, 2, 0).numpy()            # [H, W, 3]
                img_bgr = np.clip(img_bgr, 0, 255).astype(np.uint8)   # floatì¼ ìˆ˜ ìˆìœ¼ë‹ˆ uint8ë¡œ

                # ---- 2) Depth â†’ ì»¬ëŸ¬ë§µ ----
                def depth_to_color(d_tensor, max_depth=80.0):
                    d = d_tensor.squeeze(0).numpy()                   # [H, W], float
                    if np.all(d == 0):
                        norm = np.zeros_like(d, dtype=np.uint8)
                    else:
                        # 0~max_depth ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™” (ì¼ê´€ëœ ìƒ‰ ìŠ¤ì¼€ì¼)
                        d_clipped = np.clip(d, 0, max_depth) / max_depth
                        norm = (d_clipped * 255).astype(np.uint8)
                    color = cv2.applyColorMap(norm, cv2.COLORMAP_JET)
                    return color

                gt_color   = depth_to_color(label_vis)   # GT depth
                pred_color = depth_to_color(pred_vis)    # Pred depth

                # ---- 3) ì„¸ ì¥ì„ ì˜†ìœ¼ë¡œ ì´ì–´ë¶™ì´ê¸° ----
                panel = np.concatenate([img_bgr, gt_color, pred_color], axis=1)

                out_path = os.path.join(vis_dir, vis_name)
                cv2.imwrite(out_path, panel)

            except Exception as e:
                # ì‹œê°í™” ìª½ ë¬¸ì œ ë•Œë¬¸ì— í•™ìŠµì´ ë©ˆì¶”ì§€ ì•Šë„ë¡ ì¡°ìš©íˆ ë¬´ì‹œ
                pass

        with torch.no_grad():
            mae = (torch.abs(pred - label) * label_mask).sum() / (label_mask.sum() + 1e-6)
            rmse = torch.sqrt(((pred - label) ** 2 * label_mask).sum() / (label_mask.sum() + 1e-6))

        monitors = {
            "train_loss": loss.detach(),
            "mae": mae.detach(),
            "rmse": rmse.detach(),
            "flr_loss": flr_loss.detach(),
            "sgt_loss": sgt_loss.detach(),
        }

        return loss, monitors

        # ----------------------------------------------------------------

    def forward_eval(self, mini_batch_data: dict):
        """
        valid / test DataLoaderì—ì„œ ë‚˜ì˜¨ mini_batch_data í•˜ë‚˜ë¥¼ ë°›ì•„ì„œ
        - padding + device/dtype ë§ì¶”ê³ 
        - ëª¨ë¸ ì¶”ë¡ 
        - ì›ë˜ H, Wë¡œ í¬ë¡­í•´ì„œ dictë¡œ ë°˜í™˜
        """
        # ì›ë³¸ í¬ê¸° (padding í•˜ê¸° ì „ sizeë¥¼ ê¸°ì–µí•´ ë‘ )
        B, C, H, W = mini_batch_data["img"].shape

        # í•„ìˆ˜ í•­ëª©
        img        = self.padding(self.make_torch_tensor(mini_batch_data["img"],   self.device, self.dtype))
        label      = self.padding(self.make_torch_tensor(mini_batch_data["label"], self.device, self.dtype))
        label_mask = self.padding(self.make_torch_tensor(mini_batch_data["label_mask"], self.device, self.dtype))
        #pp_inst = self.padding(self.make_torch_tensor(mini_batch_data['pp_inst'], self.device, torch.long))
        #pp_sem = self.padding(self.make_torch_tensor(mini_batch_data['pp_sem'], self.device, torch.long))
        #pp_valid_mask = self.padding(self.make_torch_tensor(mini_batch_data['pp_valid_mask'], self.device, self.dtype))

 
        with torch.no_grad():
            # ë‹¨ì¼ ìŠ¤ì¼€ì¼ depth ì˜ˆì¸¡
            pred = self.forward(img)  # [B, 1, H_pad, W_pad]

        out = {
            "pred":       pred[:, :, :H, :W],       # [B, 1, H, W]
            "img":        img[:, :, :H, :W],        # [B, 3, H, W]
            "label":      label[:, :, :H, :W],      # [B, 1, H, W]
            "label_mask": label_mask[:, :, :H, :W], # [B, 1, H, W]
        }

        return out

    def forward_test(self,img):
        """
        img: [B, 3, H, W] (np.ndarray or torch.Tensor)
        return: depth_pred: [B, 1, H, W] (numpy)
        """
        B, C, H, W = img.shape

        img = self.padding(self.make_torch_tensor(img, self.device, self.dtype))

        with torch.no_grad():
            pred = self.forward(img)  # [B, 1, H_pad, W_pad]

        # padding ì „ì— ì›ë˜ í¬ê¸°ë§Œ ì˜ë¼ì„œ numpyë¡œ ë°˜í™˜
        pred = pred[:, :, :H, :W].detach().cpu().numpy()
        return pred

